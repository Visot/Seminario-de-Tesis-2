{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "    def __init__(self,input,size):\n",
    "        self.input=input\n",
    "        self.hidden_size=size\n",
    "    def :\n",
    "        # create a BasicRNNCell\n",
    "        rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n",
    "\n",
    "        # 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]\n",
    "\n",
    "        # defining initial state\n",
    "        initial_state = rnn_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "        # 'state' is a tensor of shape [batch_size, cell_state_size]\n",
    "        outputs, state = tf.nn.dynamic_rnn(rnn_cell, input_data,\n",
    "                                           initial_state=initial_state,\n",
    "                                           dtype=tf.float32)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_shape = tf.shape(input_tensor)\n",
    "n_items = input_tensor_shape[0]\n",
    "with tf.name_scope(\"lstm\"):\n",
    "        # Initialize weights\n",
    "        # with tf.device('/cpu:0'):\n",
    "        W = tf.get_variable('W', shape=[n_hidden_units, n_character],\n",
    "                            # initializer=tf.truncated_normal_initializer(stddev=h1_stddev),\n",
    "                            initializer=tf.random_normal_initializer(stddev=h1_stddev),\n",
    "                            )\n",
    "        # Initialize bias\n",
    "        # with tf.device('/cpu:0'):\n",
    "        # b = tf.get_variable('b', initializer=tf.zeros_initializer([n_character]))\n",
    "        b = tf.get_variable('b', shape=[n_character],\n",
    "                            # initializer=tf.constant_initializer(value=0),\n",
    "                            initializer=tf.random_normal_initializer(stddev=b1_stddev),\n",
    "                            )\n",
    "\n",
    "        # Define the cell\n",
    "        # Can be:\n",
    "        #   tf.contrib.rnn.BasicRNNCell\n",
    "        #   tf.contrib.rnn.GRUCell\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units, state_is_tuple=True)\n",
    "\n",
    "        # Stacking rnn cells\n",
    "        stack = tf.contrib.rnn.MultiRNNCell([cell] * n_layers, state_is_tuple=True)\n",
    "\n",
    "        # Get layer activations (second output is the final state of the layer, do not need)\n",
    "        outputs, _ = tf.nn.dynamic_rnn(stack, input_tensor, seq_length,\n",
    "                                       time_major=False, dtype=tf.float32)\n",
    "\n",
    "        # Reshape to apply the same weights over the timesteps\n",
    "        outputs = tf.reshape(outputs, [-1, n_hidden_units])\n",
    "\n",
    "        # Perform affine transformation to layer output:\n",
    "        # multiply by weights (linear transformation), add bias (translation)\n",
    "        print\n",
    "        logits = tf.add(tf.matmul(outputs, W), b)\n",
    "        \n",
    "        tf.summary.histogram(\"weights\", W)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", logits)\n",
    "\n",
    "        # Reshaping back to the original shape\n",
    "        logits = tf.reshape(logits, [n_items, -1, n_character])\n",
    "\n",
    "        # Put time as the major axis\n",
    "        logits = tf.transpose(logits, (1, 0, 2))\n",
    "\n",
    "        summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from configparser import ConfigParser\n",
    "\n",
    "from models.RNN.utils import variable_on_cpu\n",
    "\n",
    "\n",
    "def SimpleLSTM(conf_path, input_tensor, seq_length):\n",
    "    '''\n",
    "    This function was initially based on open source code from Mozilla DeepSpeech:\n",
    "    https://github.com/mozilla/DeepSpeech/blob/master/DeepSpeech.py\n",
    "    # This Source Code Form is subject to the terms of the Mozilla Public\n",
    "    # License, v. 2.0. If a copy of the MPL was not distributed with this\n",
    "    # file, You can obtain one at http://mozilla.org/MPL/2.0/.\n",
    "    '''\n",
    "    parser = ConfigParser(os.environ)\n",
    "    parser.read(conf_path)\n",
    "\n",
    "    # SimpleLSTM\n",
    "    n_character = parser.getint('simplelstm', 'n_character')\n",
    "    b1_stddev = parser.getfloat('simplelstm', 'b1_stddev')\n",
    "    h1_stddev = parser.getfloat('simplelstm', 'h1_stddev')\n",
    "    n_layers = parser.getint('simplelstm', 'n_layers')\n",
    "    n_hidden_units = parser.getint('simplelstm', 'n_hidden_units')\n",
    "\n",
    "    # Input shape: [batch_size, n_steps, n_input + 2*n_input*n_context]\n",
    "    # batch_x_shape = tf.shape(batch_x)\n",
    "\n",
    "    input_tensor_shape = tf.shape(input_tensor)\n",
    "    n_items = input_tensor_shape[0]\n",
    "\n",
    "    with tf.name_scope(\"lstm\"):\n",
    "        # Initialize weights\n",
    "        # with tf.device('/cpu:0'):\n",
    "        W = tf.get_variable('W', shape=[n_hidden_units, n_character],\n",
    "                            # initializer=tf.truncated_normal_initializer(stddev=h1_stddev),\n",
    "                            initializer=tf.random_normal_initializer(stddev=h1_stddev),\n",
    "                            )\n",
    "        # Initialize bias\n",
    "        # with tf.device('/cpu:0'):\n",
    "        # b = tf.get_variable('b', initializer=tf.zeros_initializer([n_character]))\n",
    "        b = tf.get_variable('b', shape=[n_character],\n",
    "                            # initializer=tf.constant_initializer(value=0),\n",
    "                            initializer=tf.random_normal_initializer(stddev=b1_stddev),\n",
    "                            )\n",
    "\n",
    "        # Define the cell\n",
    "        # Can be:\n",
    "        #   tf.contrib.rnn.BasicRNNCell\n",
    "        #   tf.contrib.rnn.GRUCell\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units, state_is_tuple=True)\n",
    "\n",
    "        # Stacking rnn cells\n",
    "        stack = tf.contrib.rnn.MultiRNNCell([cell] * n_layers, state_is_tuple=True)\n",
    "\n",
    "        # Get layer activations (second output is the final state of the layer, do not need)\n",
    "        outputs, _ = tf.nn.dynamic_rnn(stack, input_tensor, seq_length,\n",
    "                                       time_major=False, dtype=tf.float32)\n",
    "\n",
    "        # Reshape to apply the same weights over the timesteps\n",
    "        outputs = tf.reshape(outputs, [-1, n_hidden_units])\n",
    "\n",
    "        # Perform affine transformation to layer output:\n",
    "        # multiply by weights (linear transformation), add bias (translation)\n",
    "        logits = tf.add(tf.matmul(outputs, W), b)\n",
    "\n",
    "        tf.summary.histogram(\"weights\", W)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", logits)\n",
    "\n",
    "        # Reshaping back to the original shape\n",
    "        logits = tf.reshape(logits, [n_items, -1, n_character])\n",
    "\n",
    "        # Put time as the major axis\n",
    "        logits = tf.transpose(logits, (1, 0, 2))\n",
    "\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "    return logits, summary_op\n",
    "\n",
    "def BiRNN(conf_path, batch_x, seq_length, n_input, n_context):\n",
    "    \"\"\"\n",
    "    This function was initially based on open source code from Mozilla DeepSpeech:\n",
    "    https://github.com/mozilla/DeepSpeech/blob/master/DeepSpeech.py\n",
    "    # This Source Code Form is subject to the terms of the Mozilla Public\n",
    "    # License, v. 2.0. If a copy of the MPL was not distributed with this\n",
    "    # file, You can obtain one at http://mozilla.org/MPL/2.0/.\n",
    "    \"\"\"\n",
    "    parser = ConfigParser(os.environ)\n",
    "    parser.read(conf_path)\n",
    "\n",
    "    dropout = [float(x) for x in parser.get('birnn', 'dropout_rates').split(',')]\n",
    "    relu_clip = parser.getint('birnn', 'relu_clip')\n",
    "\n",
    "    b1_stddev = parser.getfloat('birnn', 'b1_stddev')\n",
    "    h1_stddev = parser.getfloat('birnn', 'h1_stddev')\n",
    "    b2_stddev = parser.getfloat('birnn', 'b2_stddev')\n",
    "    h2_stddev = parser.getfloat('birnn', 'h2_stddev')\n",
    "    b3_stddev = parser.getfloat('birnn', 'b3_stddev')\n",
    "    h3_stddev = parser.getfloat('birnn', 'h3_stddev')\n",
    "    b5_stddev = parser.getfloat('birnn', 'b5_stddev')\n",
    "    h5_stddev = parser.getfloat('birnn', 'h5_stddev')\n",
    "    b6_stddev = parser.getfloat('birnn', 'b6_stddev')\n",
    "    h6_stddev = parser.getfloat('birnn', 'h6_stddev')\n",
    "\n",
    "    n_hidden_1 = parser.getint('birnn', 'n_hidden_1')\n",
    "    n_hidden_2 = parser.getint('birnn', 'n_hidden_2')\n",
    "    n_hidden_5 = parser.getint('birnn', 'n_hidden_5')\n",
    "    n_cell_dim = parser.getint('birnn', 'n_cell_dim')\n",
    "\n",
    "    n_hidden_3 = int(eval(parser.get('birnn', 'n_hidden_3')))\n",
    "    n_hidden_6 = parser.getint('birnn', 'n_hidden_6')\n",
    "\n",
    "    # Input shape: [batch_size, n_steps, n_input + 2*n_input*n_context]\n",
    "    batch_x_shape = tf.shape(batch_x)\n",
    "\n",
    "    # Reshaping `batch_x` to a tensor with shape `[n_steps*batch_size, n_input + 2*n_input*n_context]`.\n",
    "    # This is done to prepare the batch for input into the first layer which expects a tensor of rank `2`.\n",
    "\n",
    "    # Permute n_steps and batch_size\n",
    "    batch_x = tf.transpose(batch_x, [1, 0, 2])\n",
    "    # Reshape to prepare input for first layer\n",
    "    batch_x = tf.reshape(batch_x,\n",
    "                         [-1, n_input + 2 * n_input * n_context])  # (n_steps*batch_size, n_input + 2*n_input*n_context)\n",
    "\n",
    "    # The next three blocks will pass `batch_x` through three hidden layers with\n",
    "    # clipped RELU activation and dropout.\n",
    "\n",
    "    # 1st layer\n",
    "    with tf.name_scope('fc1'):\n",
    "        b1 = variable_on_cpu('b1', [n_hidden_1], tf.random_normal_initializer(stddev=b1_stddev))\n",
    "        h1 = variable_on_cpu('h1', [n_input + 2 * n_input * n_context, n_hidden_1],\n",
    "                             tf.random_normal_initializer(stddev=h1_stddev))\n",
    "        layer_1 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(batch_x, h1), b1)), relu_clip)\n",
    "        layer_1 = tf.nn.dropout(layer_1, (1.0 - dropout[0]))\n",
    "\n",
    "        tf.summary.histogram(\"weights\", h1)\n",
    "        tf.summary.histogram(\"biases\", b1)\n",
    "        tf.summary.histogram(\"activations\", layer_1)\n",
    "\n",
    "    # 2nd layer\n",
    "    with tf.name_scope('fc2'):\n",
    "        b2 = variable_on_cpu('b2', [n_hidden_2], tf.random_normal_initializer(stddev=b2_stddev))\n",
    "        h2 = variable_on_cpu('h2', [n_hidden_1, n_hidden_2], tf.random_normal_initializer(stddev=h2_stddev))\n",
    "        layer_2 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(layer_1, h2), b2)), relu_clip)\n",
    "        layer_2 = tf.nn.dropout(layer_2, (1.0 - dropout[1]))\n",
    "\n",
    "        tf.summary.histogram(\"weights\", h2)\n",
    "        tf.summary.histogram(\"biases\", b2)\n",
    "        tf.summary.histogram(\"activations\", layer_2)\n",
    "\n",
    "    # 3rd layer\n",
    "    with tf.name_scope('fc3'):\n",
    "        b3 = variable_on_cpu('b3', [n_hidden_3], tf.random_normal_initializer(stddev=b3_stddev))\n",
    "        h3 = variable_on_cpu('h3', [n_hidden_2, n_hidden_3], tf.random_normal_initializer(stddev=h3_stddev))\n",
    "        layer_3 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(layer_2, h3), b3)), relu_clip)\n",
    "        layer_3 = tf.nn.dropout(layer_3, (1.0 - dropout[2]))\n",
    "\n",
    "        tf.summary.histogram(\"weights\", h3)\n",
    "        tf.summary.histogram(\"biases\", b3)\n",
    "        tf.summary.histogram(\"activations\", layer_3)\n",
    "\n",
    "    # Create the forward and backward LSTM units. Inputs have length `n_cell_dim`.\n",
    "    # LSTM forget gate bias initialized at `1.0` (default), meaning less forgetting\n",
    "    # at the beginning of training (remembers more previous info)\n",
    "    with tf.name_scope('lstm'):\n",
    "        # Forward direction cell:\n",
    "        lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(n_cell_dim, forget_bias=1.0, state_is_tuple=True)\n",
    "        lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(lstm_fw_cell,\n",
    "                                                     input_keep_prob=1.0 - dropout[3],\n",
    "                                                     output_keep_prob=1.0 - dropout[3],\n",
    "                                                     # seed=random_seed,\n",
    "                                                     )\n",
    "        # Backward direction cell:\n",
    "        lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(n_cell_dim, forget_bias=1.0, state_is_tuple=True)\n",
    "        lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(lstm_bw_cell,\n",
    "                                                     input_keep_prob=1.0 - dropout[4],\n",
    "                                                     output_keep_prob=1.0 - dropout[4],\n",
    "                                                     # seed=random_seed,\n",
    "                                                     )\n",
    "\n",
    "        # `layer_3` is now reshaped into `[n_steps, batch_size, 2*n_cell_dim]`,\n",
    "        # as the LSTM BRNN expects its input to be of shape `[max_time, batch_size, input_size]`.\n",
    "        layer_3 = tf.reshape(layer_3, [-1, batch_x_shape[0], n_hidden_3])\n",
    "\n",
    "        # Now we feed `layer_3` into the LSTM BRNN cell and obtain the LSTM BRNN output.\n",
    "        outputs, output_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell,\n",
    "                                                                 cell_bw=lstm_bw_cell,\n",
    "                                                                 inputs=layer_3,\n",
    "                                                                 dtype=tf.float32,\n",
    "                                                                 time_major=True,\n",
    "                                                                 sequence_length=seq_length)\n",
    "\n",
    "        tf.summary.histogram(\"activations\", outputs)\n",
    "\n",
    "        # Reshape outputs from two tensors each of shape [n_steps, batch_size, n_cell_dim]\n",
    "        # to a single tensor of shape [n_steps*batch_size, 2*n_cell_dim]\n",
    "        outputs = tf.concat(outputs, 2)\n",
    "        outputs = tf.reshape(outputs, [-1, 2 * n_cell_dim])\n",
    "\n",
    "    with tf.name_scope('fc5'):\n",
    "        # Now we feed `outputs` to the fifth hidden layer with clipped RELU activation and dropout\n",
    "        b5 = variable_on_cpu('b5', [n_hidden_5], tf.random_normal_initializer(stddev=b5_stddev))\n",
    "        h5 = variable_on_cpu('h5', [(2 * n_cell_dim), n_hidden_5], tf.random_normal_initializer(stddev=h5_stddev))\n",
    "        layer_5 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(outputs, h5), b5)), relu_clip)\n",
    "        layer_5 = tf.nn.dropout(layer_5, (1.0 - dropout[5]))\n",
    "\n",
    "        tf.summary.histogram(\"weights\", h5)\n",
    "        tf.summary.histogram(\"biases\", b5)\n",
    "        tf.summary.histogram(\"activations\", layer_5)\n",
    "\n",
    "    with tf.name_scope('fc6'):\n",
    "        # Now we apply the weight matrix `h6` and bias `b6` to the output of `layer_5`\n",
    "        # creating `n_classes` dimensional vectors, the logits.\n",
    "        b6 = variable_on_cpu('b6', [n_hidden_6], tf.random_normal_initializer(stddev=b6_stddev))\n",
    "        h6 = variable_on_cpu('h6', [n_hidden_5, n_hidden_6], tf.random_normal_initializer(stddev=h6_stddev))\n",
    "        layer_6 = tf.add(tf.matmul(layer_5, h6), b6)\n",
    "\n",
    "        tf.summary.histogram(\"weights\", h6)\n",
    "        tf.summary.histogram(\"biases\", b6)\n",
    "        tf.summary.histogram(\"activations\", layer_6)\n",
    "\n",
    "    # Finally we reshape layer_6 from a tensor of shape [n_steps*batch_size, n_hidden_6]\n",
    "    # to the slightly more useful shape [n_steps, batch_size, n_hidden_6].\n",
    "    # Note, that this differs from the input in that it is time-major.\n",
    "    layer_6 = tf.reshape(layer_6, [-1, batch_x_shape[0], n_hidden_6])\n",
    "\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    # Output shape: [n_steps, batch_size, n_hidden_6]\n",
    "return layer_6, summary_op"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
